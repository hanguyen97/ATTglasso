typos <- setdiff(v2, dict)
typos[grepl("^respect", typos)]
dist_v = adist("respecfull", dict, partial = FALSE)
dict[dist_v <= 2]
dict[dist_v <= 1]
dist_v = adist("respecfull", dict, partial = FALSE)
dict[dist_v <= 2]
?adist
?words
?adist
agrep(pattern = "respectful", x = typos, max.distance = 0.1, value = TRUE)
?agrep
?adist
dist_v = adist("respecfull", dict, partial = FALSE)
dict[dist_v <= 2]
s = "respectul"
t = "respectful"
out <- adist(x = s, y = t, counts = TRUE)
attributes(out)$count[1,1,]
s = "respectsul"
t = "respectful"
out <- adist(x = s, y = t, counts = TRUE)
attributes(out)$count[1,1,]
s = "resectsul"
t = "respectful"
out <- adist(x = s, y = t, counts = TRUE)
attributes(out)$count[1,1,]
s = "reectful"
t = "respectful"
out <- adist(x = s, y = t, counts = TRUE)
attributes(out)$count[1,1,]
s = "respectul"
t = "respectful"
out <- adist(x = s, y = t, counts = TRUE)
attributes(out)$count[1,1,]
s = "respectul"
t = "respectful"
out <- adist(x = s, y = t, counts = TRUE)
s = "respectul"
t = "respectful"
out <- adist(x = s, y = t, counts = TRUE)
out
s = "respectul"
t = "respectful"
adist(x = s, y = t, counts = TRUE)
s = "respectul"
t = "respectful"
out <- adist(x = s, y = t, counts = TRUE)
attributes(out)$count[1,1,]
knitr::opts_chunk$set(echo = TRUE)
dat.all <- read.csv("~/Documents/Cornell/TA/TextAnalysis/Course1/Quiz/Airbnb_Open_Data.csv", header = TRUE, check.names = FALSE)
nrow(dat.all) # Check number of observations
names(dat.all) # Check all columns
# Subset to useful columns
dat <- dat.all[dat.all$country=="United States" & dat.all$house_rules!="",]
dat <- dat[!grepl("[\u4E00-\u9FFF]", dat$house_rules), ]
dat <- dat[!is.na(dat$house_rules),]
# Remove duplicate rows
dat <- unique(dat)
# Check the data
nrow(dat)
head(dat)
library(words)
dict = words[,1]
v = dat$house_rules
v2 = unlist(strsplit(v, split = " |\\.|,|\\\"|;|\\(|\\)|!|/|'s|:"))
v2 = tolower(v2)
v2 = sort(unique(v2))
v2 = v2[-seq(7)]
typos <- setdiff(v2, dict)
typos[grepl("^respect", typos)]
s = "respectul"
t = "respectful"
out <- adist(x = s, y = t, counts = TRUE)
attributes(out)$count[1,1,]
# adist("respecfull", dict, partial = FALSE) <= 2
# dist_v = adist("respecfull", dict, partial = FALSE)
# dict[dist_v <= 2]
# agrep("respecfull", dict, max.distance = 2, value = TRUE)
# grep("respecfull", dict, fixed = TRUE, value = TRUE)
agrep(pattern = "respectful", x = typos, max.distance = 0.1, value = TRUE)
smoke_words <- v2[grepl("^smok", v2)]
toks <- quanteda::tokens(smoke_words, remove_punct = TRUE)
toks
stem <- tokens_wordstem(toks)
stem
smoke_words <- v2[grepl("^smok", v2)]
toks <- quanteda::tokens(smoke_words, remove_punct = TRUE)
stem <- tokens_wordstem(toks)
stem
stem[7:9]
unique(stem)
smoke_words <- v2[grepl("^smok", v2)]
toks <- quanteda::tokens(smoke_words, remove_punct = TRUE)
stem <- tokens_wordstem(toks)
stem
?tokens_wordstem
wordStem("winning")
wordStem(c("helped", "helpful", "help"))
smoke_words <- v2[grepl("^smok", v2)]
wordStem(smoke_words)
# limitations of stemming
char_wordstem(c("going", "went"))
library(lexicon)
hash_lemmas$lemma[hash_lemmas$token == "went"]
hash_lemmas$lemma[hash_lemmas$token == "cities"]
library(lexicon)
# (hash_lemmas)
toks <- tokens(smoke_words)
tokens_replace(toks,
pattern = lexicon::hash_lemmas$token,
replacement = lexicon::hash_lemmas$lemma)
library(lexicon)
toks <- tokens(smoke_words)
tokens_replace(toks,
pattern = lexicon::hash_lemmas$token,
replacement = lexicon::hash_lemmas$lemma)
knitr::opts_chunk$set(echo = TRUE)
# Load necessary libraries
library(quanteda)
library(readtext)
library(SnowballC)
library(newsmap)
dat.all <- read.csv("~/Documents/Cornell/TA/TextAnalysis/Course1/Quiz/Airbnb_Open_Data.csv", header = TRUE, check.names = FALSE)
nrow(dat.all) # Check number of observations
names(dat.all) # Check all columns
# Subset to useful columns
dat <- dat.all[dat.all$country=="United States" & dat.all$house_rules!="",]
dat <- dat[!grepl("[\u4E00-\u9FFF]", dat$house_rules), ]
dat <- dat[!is.na(dat$house_rules),]
# Remove duplicate rows
dat <- unique(dat)
# Check the data
nrow(dat)
head(dat)
library(words)
dict = words[,1]
v = dat$house_rules
v2 = unlist(strsplit(v, split = " |\\.|,|\\\"|;|\\(|\\)|!|/|'s|:"))
v2 = tolower(v2)
v2 = sort(unique(v2))
v2 = v2[-seq(7)]
# library(SnowballC)
# smoke_words <- v2[grepl("^smok", v2)]
# YOUR FUNCTION HERE(smoke_words)
library(SnowballC)
smoke_words <- v2[grepl("^smok", v2)]
wordStem(smoke_words)
# library(lexicon)
# toks <- tokens(smoke_words)
# tokens_replace(toks,
#                pattern = YOUR CODE HERE,
#                replacement = YOUR CODE HERE)
library(lexicon)
toks <- tokens(smoke_words)
tokens_replace(toks,
pattern = lexicon::hash_lemmas$token,
replacement = lexicon::hash_lemmas$lemma)
# library(textstem)
# YOUR FUNCTION HERE(smoke_words)
library(textstem)
lemmatize_words(smoke_words)
knitr::opts_chunk$set(echo = TRUE)
# Load necessary libraries
library(quanteda)
library(readtext)
library(SnowballC)
library(newsmap)
# ## CODE TO SUBSET REVIEWS DATA
# set.seed(1)
# dat.all <- read.csv("~/Documents/Cornell/TA/TextAnalysis/Project/Course1/Reviews.csv", header = TRUE, check.names = FALSE)
# # Subset to useful columns
# dat <- dat.all[, c("ProductId", "UserId", "Score", "Time", "Summary", "Text")]
# # Remove duplicate rows
# dat <- unique(dat)
#
# delicious_mispelled <- grepl(pattern = "delishus|delisious|delicius|delishious|deliciouz", x = dat$Text)
# mispelled_obs <- which(delicious_mispelled == TRUE)
# reduce_reviews <- dat[c(mispelled_obs, sample(1:nrow(dat.all), 1000 - length(mispelled_obs))),]
# reduce_reviews <- reduce_reviews[order(reduce_reviews$ProductId, reduce_reviews$UserId), ]
# write.csv(reduce_reviews, file="~/Documents/Cornell/TA/TextAnalysis/Project/Course1/Reviews_reduced.csv")
dat.all <- read.csv("~/Documents/Cornell/TA/TextAnalysis/Course1/Project/Reviews_reduced.csv", header = TRUE, check.names = FALSE)
nrow(dat.all) # Check number of observations
names(dat.all) # Check all columns
# Subset to useful columns
dat <- dat.all[, c("ProductId", "UserId", "Score", "Time", "Summary", "Text")]
# Convert time to POSIX date-time class
dat$Time <- as.POSIXct(dat$Time, origin = "1970-01-01")
# Remove duplicate rows
dat <- unique(dat)
# Check the data
nrow(dat)
head(dat)
s = "delishious"
t = "delicious"
adist(x = s, y = t)
out <- adist(x = s, y = t, counts = TRUE, partial = TRUE)
attributes(out)$count[1,1,]
# library(words)
# dict = words[,1]
#
# v = dat$Text
# v2 = YOUR CODE HERE(v, split = " |\\.|,|;|\\(|\\)|!|/|'s|:"))
# v2 = tolower(v2)
# v2 = sort(unique(v2))
# v2 = v2[-seq(7)]
#
# typos <- YOUR CODE HERE
# typos[grepl(YOUR CODE HERE, typos)]
# YOUR CODE HERE(, , max.distance = 0.1, value = TRUE)
library(words)
dict = words[,1]
v = dat$Text
v2 = unlist(strsplit(v, split = " |\\.|,|;|\\(|\\)|!|/|'s|:"))
v2 = tolower(v2)
v2 = sort(unique(v2))
v2 = v2[-seq(7)]
typos <- setdiff(v2, dict)
typos[grepl("^deli", typos)]
agrep(pattern = "delicious", x = typos, max.distance = 0.1, value = TRUE)
# purchase_words <- v2[grepl("^purchas", v2)]
# YOUR CODE HERE(purchase_words)
# library(lexicon)
# # View(hash_lemmas)
# YOUR CODE HERE(toks, pattern = YOUR CODE HERE, replacement = YOUR CODE HERE)
#
# library(textstem)
# YOUR CODE HERE(purchase_words)
purchase_words <- v2[grepl("^purchas", v2)]
wordStem(purchase_words)
library(lexicon)
# View(hash_lemmas)
tokens_replace(toks,
pattern = lexicon::hash_lemmas$token,
replacement = lexicon::hash_lemmas$lemma)
library(textstem)
lemmatize_words(purchase_words)
# dict <- YOUR CODE HERE
# rev <- dat$Summary[1:200]
# toks <- quanteda::YOUR CODE HERE
# dict_toks <- tokens_lookup(YOUR CODE HERE)
#
# # Find non-empty element
# idx <- which(sapply(dict_toks, length) > 0)
# print(idx)
# toks[idx]
dict <- dictionary(list(healthy = c("organic", "gluten-free", "plant-based", "nutritious")))
rev <- dat$Summary[1:200]
toks <- quanteda::tokens(rev)
dict_toks <- tokens_lookup(quanteda::tokens(rev), dictionary = dict)
# Find non-empty element
idx <- which(sapply(dict_toks, length) > 0)
print(idx)
toks[idx]
library(lexicon)
# View(hash_lemmas)
toks <- tokens(purchase_words)
library(lexicon)
# View(hash_lemmas)
toks <- quanteda::tokens(purchase_words)
tokens_replace(toks,
pattern = lexicon::hash_lemmas$token,
replacement = lexicon::hash_lemmas$lemma)
library(textstem)
lemmatize_words(purchase_words)
knitr::opts_chunk$set(echo = TRUE)
# Load necessary libraries
library(quanteda)
library(readtext)
library(SnowballC)
library(newsmap)
# ## CODE TO SUBSET REVIEWS DATA
# set.seed(1)
# dat.all <- read.csv("~/Documents/Cornell/TA/TextAnalysis/Project/Course1/Reviews.csv", header = TRUE, check.names = FALSE)
# # Subset to useful columns
# dat <- dat.all[, c("ProductId", "UserId", "Score", "Time", "Summary", "Text")]
# # Remove duplicate rows
# dat <- unique(dat)
#
# delicious_mispelled <- grepl(pattern = "delishus|delisious|delicius|delishious|deliciouz", x = dat$Text)
# mispelled_obs <- which(delicious_mispelled == TRUE)
# reduce_reviews <- dat[c(mispelled_obs, sample(1:nrow(dat.all), 1000 - length(mispelled_obs))),]
# reduce_reviews <- reduce_reviews[order(reduce_reviews$ProductId, reduce_reviews$UserId), ]
# write.csv(reduce_reviews, file="~/Documents/Cornell/TA/TextAnalysis/Project/Course1/Reviews_reduced.csv")
dat.all <- read.csv("~/Documents/Cornell/TA/TextAnalysis/Course1/Project/Reviews_reduced.csv", header = TRUE, check.names = FALSE)
nrow(dat.all) # Check number of observations
names(dat.all) # Check all columns
# Subset to useful columns
dat <- dat.all[, c("ProductId", "UserId", "Score", "Time", "Summary", "Text")]
# Convert time to POSIX date-time class
dat$Time <- as.POSIXct(dat$Time, origin = "1970-01-01")
# Remove duplicate rows
dat <- unique(dat)
# Check the data
nrow(dat)
head(dat)
s = "delishious"
t = "delicious"
adist(x = s, y = t)
out <- adist(x = s, y = t, counts = TRUE, partial = TRUE)
attributes(out)$count[1,1,]
# library(words)
# dict = words[,1]
#
# v = dat$Text
# v2 = YOUR CODE HERE(v, split = " |\\.|,|;|\\(|\\)|!|/|'s|:"))
# v2 = tolower(v2)
# v2 = sort(unique(v2))
# v2 = v2[-seq(7)]
#
# typos <- YOUR CODE HERE
# typos[grepl(YOUR CODE HERE, typos)]
# YOUR CODE HERE(, , max.distance = 0.1, value = TRUE)
library(words)
dict = words[,1]
v = dat$Text
v2 = unlist(strsplit(v, split = " |\\.|,|;|\\(|\\)|!|/|'s|:"))
v2 = tolower(v2)
v2 = sort(unique(v2))
v2 = v2[-seq(7)]
typos <- setdiff(v2, dict)
typos[grepl("^deli", typos)]
agrep(pattern = "delicious", x = typos, max.distance = 0.1, value = TRUE)
# purchase_words <- v2[grepl("^purchas", v2)]
# YOUR CODE HERE(purchase_words)
# library(lexicon)
# # View(hash_lemmas)
# toks <- quanteda::tokens(purchase_words)
# YOUR CODE HERE(toks, pattern = YOUR CODE HERE, replacement = YOUR CODE HERE)
#
# library(textstem)
# YOUR CODE HERE(purchase_words)
purchase_words <- v2[grepl("^purchas", v2)]
wordStem(purchase_words)
library(lexicon)
# View(hash_lemmas)
toks <- quanteda::tokens(purchase_words)
tokens_replace(toks,
pattern = lexicon::hash_lemmas$token,
replacement = lexicon::hash_lemmas$lemma)
library(textstem)
lemmatize_words(purchase_words)
# dict <- YOUR CODE HERE
# rev <- dat$Summary[1:200]
# toks <- quanteda::YOUR CODE HERE
# dict_toks <- tokens_lookup(YOUR CODE HERE)
#
# # Find non-empty element
# idx <- which(sapply(dict_toks, length) > 0)
# print(idx)
# toks[idx]
dict <- dictionary(list(healthy = c("organic", "gluten-free", "plant-based", "nutritious")))
rev <- dat$Summary[1:200]
toks <- quanteda::tokens(rev)
dict_toks <- tokens_lookup(quanteda::tokens(rev), dictionary = dict)
# Find non-empty element
idx <- which(sapply(dict_toks, length) > 0)
print(idx)
toks[idx]
knitr::opts_chunk$set(echo = TRUE)
# Load necessary libraries
library(quanteda)
library(readtext)
library(SnowballC)
library(newsmap)
dat.all <- read.csv("~/Documents/Cornell/TA/TextAnalysis/Course1/Quiz/Airbnb_Open_Data.csv", header = TRUE, check.names = FALSE)
nrow(dat.all) # Check number of observations
names(dat.all) # Check all columns
# Subset to useful columns
dat <- dat.all[dat.all$country=="United States" & dat.all$house_rules!="",]
dat <- dat[!grepl("[\u4E00-\u9FFF]", dat$house_rules), ]
dat <- dat[!is.na(dat$house_rules),]
# Remove duplicate rows
dat <- unique(dat)
# Check the data
nrow(dat)
head(dat)
library(words)
dict = words[,1]
v = dat$house_rules
v2 = unlist(strsplit(v, split = " |\\.|,|\\\"|;|\\(|\\)|!|/|'s|:"))
v2 = tolower(v2)
v2 = sort(unique(v2))
v2 = v2[-seq(7)]
dict <- dictionary(list(pet = c("pet", "pets", "cat", "cats", "dog", "dogs")))
rev <- dat$house_rules[1:10]
toks <- quanteda::tokens(rev)
dict_toks <- tokens_lookup(quanteda::tokens(rev), dictionary = dict)
dict_toks
# Check an example
dict_toks[2]
dat$house_rules[2]
# Find non-empty element
idx <- which(sapply(dict_toks, length) > 0)
print(idx)
toks[idx]
# Find non-empty element
idx <- which(sapply(dict_toks, length) > 0)
print(idx)
rev[idx]
View(dat$house_rules)
View(dat)
# dict <- dictionary(list(pet = c("pet", "pets", "cat", "cats", "dog", "dogs")))
dict <- dictionary(list(pet = c("breakfast", "snacks", "coffee", "refreshment")))
rules <- dat$house_rules[1:10]
dict_toks <- tokens_lookup(quanteda::tokens(rules), dictionary = dict)
dict_toks
# Find non-empty element
idx <- which(sapply(dict_toks, length) > 0)
print(idx)
# dict <- dictionary(list(pet = c("pet", "pets", "cat", "cats", "dog", "dogs")))
dict <- dictionary(list(pet = c("breakfast", "snacks", "coffee", "refreshment")))
rules <- dat$house_rules[1:50]
dict_toks <- tokens_lookup(quanteda::tokens(rules), dictionary = dict)
dict_toks
# Find non-empty element
idx <- which(sapply(dict_toks, length) > 0)
print(idx)
rules[idx]
# dict <- dictionary(list(pet = c("pet", "pets", "cat", "cats", "dog", "dogs")))
dict <- dictionary(list(pet = c("wifi", "towels", "hairdryer", "toiletries", "heating")))
rules <- dat$house_rules[1:50]
dict_toks <- tokens_lookup(quanteda::tokens(rules), dictionary = dict)
dict_toks
# Find non-empty element
idx <- which(sapply(dict_toks, length) > 0)
print(idx)
rules[idx]
# Find non-empty element
idx <- which(sapply(dict_toks, length) > 0)
print(idx)
rules[idx[1]]
dict <- dictionary(list(pet = c("wi-fi", "towels", "hairdryer", "toiletries", "heating")))
rules <- dat$house_rules[1:50]
dict_toks <- tokens_lookup(quanteda::tokens(rules), dictionary = dict)
dict_toks
# Find non-empty element
idx <- which(sapply(dict_toks, length) > 0)
print(idx)
rules[idx[1]]
dict <- dictionary(list(pet = c("Wi-Fi", "towels", "hairdryer", "toiletries", "heating")))
rules <- dat$house_rules[1:50]
dict_toks <- tokens_lookup(quanteda::tokens(rules), dictionary = dict)
dict_toks
# Find non-empty element
idx <- which(sapply(dict_toks, length) > 0)
print(idx)
rules[idx[1]]
dict <- dictionary(list(pet = c("wifi", "towels", "hairdryer", "toiletries", "heating")))
rules <- dat$house_rules[1:50]
dict_toks <- tokens_lookup(quanteda::tokens(rules), dictionary = dict)
dict_toks
# Find non-empty element
idx <- which(sapply(dict_toks, length) > 0)
print(idx)
rules[idx[1]]
rules <- dat$house_rules[1:50]
dict_toks <- tokens_lookup(quanteda::tokens(rules), dictionary = dict)
# Which house rules mention the amenities
idx <- which(sapply(dict_toks, length) > 0)
length(idx)
knitr::opts_chunk$set(echo = TRUE)
# Load necessary libraries
library(quanteda)
library(readtext)
library(SnowballC)
library(newsmap)
dat.all <- read.csv("~/Documents/Cornell/TA/TextAnalysis/Course1/Quiz/Airbnb_Open_Data.csv", header = TRUE, check.names = FALSE)
nrow(dat.all) # Check number of observations
names(dat.all) # Check all columns
# Subset to useful columns
dat <- dat.all[dat.all$country=="United States" & dat.all$house_rules!="",]
dat <- dat[!grepl("[\u4E00-\u9FFF]", dat$house_rules), ]
dat <- dat[!is.na(dat$house_rules),]
# Remove duplicate rows
dat <- unique(dat)
# Check the data
nrow(dat)
head(dat)
library(words)
dict = words[,1]
v = dat$house_rules
v2 = unlist(strsplit(v, split = " |\\.|,|\\\"|;|\\(|\\)|!|/|'s|:"))
v2 = tolower(v2)
v2 = sort(unique(v2))
v2 = v2[-seq(7)]
# items <- c("wifi", "towels", "hairdryer", "toiletries", "heating", "air conditioner")
items <- c("wifi", "towels", "hairdryer", "toiletries", "heating", "air conditioner")
dict <- dictionary(list(amenities = items))
# rules <- dat$house_rules[1:50]
# dict_toks <- YOUR CODE HERE
# # Which house rules mention the amenities
# idx <- which(sapply(dict_toks, length) > 0)
# length(idx)
rules <- dat$house_rules[1:50]
dict_toks <- tokens_lookup(quanteda::tokens(rules), dictionary = dict)
# Which house rules mention the amenities
idx <- which(sapply(dict_toks, length) > 0)
length(idx)
?tokens_lookup
knitr::opts_chunk$set(echo = TRUE)
# Load necessary libraries
library(quanteda)
rules <- dat$house_rules[1:50]
# dict_toks <- tokens_lookup(quanteda::tokens(rules), dictionary = dict)
dict_toks <- tokens_lookup(rules, dictionary = dict)
rules <- dat$house_rules[1:50]
toks <- quanteda::tokens(rules)
dict_toks <- tokens_lookup(toks, dictionary = dict)
# Which house rules mention the amenities
idx <- which(sapply(dict_toks, length) > 0)
length(idx)
